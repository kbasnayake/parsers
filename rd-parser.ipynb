{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Descent Parser\n",
    "\n",
    "A **recursive descent parser** is a kind of top-down parser built from a set of mutually recursive procedures where each such procedure implements one of the nonterminals of the grammar.\n",
    "\n",
    "Thus the structure of the resulting program closely mirrors that of the grammar it recognizes. A predictive parser is a recursive descent parser that does not require backtracking. Predictive parsing is possible only for the class of *LL grammars*, which are the context-free grammars for which there exists some positive integer *k* that allows a recursive descent parser to decide which production to use by examining only the next *k* tokens of input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load some auxiliary tools\n",
    "#load \"grammartools.fsx\"\n",
    "open CSCI374.GrammarTools\n",
    "open CSCI374.ParserTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's defined a class for a lexical analyzer `Tokenizer` which will facilitate operations with the token stream.\n",
    "- All necessary types are defined in [grammartools.fsx](grammartools.fsx) script (e.g `PRODUCTION`, `TOKEN`, and etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type Tokenizer(grammar: PRODUCTION [], verbose: bool) =\n",
    "    let mutable inputState = []\n",
    "    let mutable curentToken = INVALID\n",
    "\n",
    "    // Access to the\n",
    "    member this.CurrentToken = curentToken    \n",
    "    member this.NextToken() =\n",
    "        let tkn, input = CSCI374.Lexer.token inputState\n",
    "        inputState <- input\n",
    "        curentToken <- tkn\n",
    "        this\n",
    "    \n",
    "    member this.InputState\n",
    "        with set(str) = inputState <- Seq.toList str\n",
    "    member this.IsVerbose = verbose\n",
    "    member this.PrintRule ruleIdx =\n",
    "        printGrammarRule false grammar ruleIdx // print rule\n",
    "    new(grammar) = Tokenizer(grammar, false)\n",
    "    \n",
    "\n",
    "/// This infix operator function provides verbose output while calling\n",
    "/// a particular production rule\n",
    "let (==>) (cnxt:Tokenizer) (prod:Tokenizer->Tokenizer) =\n",
    "    if cnxt.IsVerbose then\n",
    "        printfn \"Enter <%A> with token `%A`\" prod cnxt.CurrentToken\n",
    "    let nextcnxt = prod cnxt\n",
    "    if cnxt.IsVerbose then\n",
    "        printfn \"Exit <%A> with token `%A`\" prod cnxt.CurrentToken\n",
    "    nextcnxt\n",
    "    \n",
    "/// This infix operator function will allow to print a production rule\n",
    "/// call `cnxt @ 2` will print second grammar rule\n",
    "let (@) (cnxt:Tokenizer) ruleIdx =\n",
    "    cnxt.PrintRule ruleIdx\n",
    "    cnxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a recursive descent parser for a following grammar\n",
    "```\n",
    "S → T | ( S + T )\n",
    "T → a\n",
    "```\n",
    "\n",
    "This is LL grammar as it doesn't have left recursion and every production passes pairwise disjointness test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [grammartools.fsx](grammartools.fsx) script provides a function `parseGrammarString` that parses a string representation of the grammar in to array of production rules that we can for our parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|(NonTerminal S, [NonTerminal T]);\n",
      "  (NonTerminal S,\n",
      "   [Terminal LPAR; NonTerminal S; Terminal PLUS; NonTerminal T; Terminal RPAR]);\n",
      "  (NonTerminal T, [Terminal A])|]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<null>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let grammar = parseGrammarString \"\"\"\n",
    "    S → T | ( S + T )\n",
    "    T → a\n",
    "\"\"\"\n",
    "printfn \"%A\" grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar:\n",
      "1: S → T\n",
      "2: S → (S+T)\n",
      "3: T → a\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<null>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Show grammar rules\n",
    "printGrammar grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we write a recursive descent parser? There is a recursive function for each nonterminal in the grammar, which can parse sentences that can be generated by that nonterminal:\n",
    "\n",
    "1. Assume that the grammar rule has only one right hand side (RHS) production (e.g. `T → a`):\n",
    "    - For each terminal symbol in the RHS, compare it with the next input token, and if they match, continue, else there is an error. We call this function `Match`.\n",
    "    - For each nonterminal symbol in the RHS, call its associated parsing function. We call these functions `Prod`.\n",
    "2. Assume that the grammar rule has more than one right hand side (RHS) requires an initial process to determine which RHS it is to parse (e.g. `S → T | ( S + T )`):\n",
    "    - The correct RHS is chosen on the basis of the next token of input (the lookahead)\n",
    "    - The next token is compared with the first token that can be generated by each RHS until a match is found\n",
    "    - If no match is found, it is a syntax error\n",
    "\n",
    "Let's write these recursive function for the above grammar.\n",
    "- F# provides a specific syntax for [mutually recursive functions](https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/functions/recursive-functions-the-rec-keyword#mutually-recursive-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Start with a production S → T | ( S + T )\n",
    "/// Let's enumerate rules as follows\n",
    "/// 1: S → T\n",
    "/// 2: S → ( S + T )\n",
    "/// because RHS productions are pairwise disjoint, the rule #2 is always starts with matching `(`\n",
    "let rec ProdS (cnxt:Tokenizer) =\n",
    "    // check the current token is `(` then select rule #2\n",
    "    if cnxt.CurrentToken = LPAR then\n",
    "        // 2: S -> ( S + T )\n",
    "        cnxt.NextToken() @(2)==> ProdS ==> Match PLUS ==> ProdT ==> Match RPAR \n",
    "    else\n",
    "        // 1: S -> T\n",
    "        cnxt @(1)==> ProdT \n",
    "\n",
    "/// The function for production T → a is straight forward: match nonterminal `a`\n",
    "and ProdT (cnxt:Tokenizer) =\n",
    "    // 3: T -> a\n",
    "    cnxt @(3)==> Match A\n",
    "\n",
    "/// For each terminal symbol compare it with a current token\n",
    "/// and if they match, continue with the next token, else there is an error\n",
    "and Match term cnxt =\n",
    "    if cnxt.IsVerbose then printfn \"Match %A with %A\" term cnxt.CurrentToken\n",
    "    // if we matched the current token with a terminal symbol\n",
    "    if term = cnxt.CurrentToken then\n",
    "        cnxt.NextToken() // read next token\n",
    "    else\n",
    "        failwith (sprintf \"Cannot match symbol `%A` with `%A`\" term cnxt.CurrentToken)\n",
    "    \n",
    "/// Start parsing by calling starting symbol function\n",
    "let parser (cnxt:Tokenizer) :Tokenizer =    \n",
    "    // Read token and pass it to the function for S rule\n",
    "    cnxt.NextToken() ==> ProdS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string to parse is `((a+a)+a)`. We run this string through the lexical analyzer to see a tokenized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LPAR; LPAR; A; PLUS; A; RPAR; PLUS; A; RPAR; END]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<null>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let inputString = \"((a+a)+a)\"\n",
    "inputString |> Seq.toList |> CSCI374.Lexer.tokenize |> printfn \"%A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer object and begin parsing. We should be able to see a sequence of grammar rules required to parse the above expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: S → (S+T)\n",
      "2: S → (S+T)\n",
      "1: S → T\n",
      "3: T → a\n",
      "3: T → a\n",
      "3: T → a\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<null>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer(grammar, InputState=inputString) |> parser |> ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if a string is not generated by our grammar: `((a+)+a)`. In addition, we turn on more verbose parser output by setting second parameter of `Tokenizer` to `true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter <<fun:it@1-1>> with token `LPAR`\n",
      "2: S → (S+T)\n",
      "Enter <<fun:ProdS@10>> with token `LPAR`\n",
      "2: S → (S+T)\n",
      "Enter <<fun:ProdS@10>> with token `A`\n",
      "1: S → T\n",
      "Enter <<fun:ProdS@13-4>> with token `A`\n",
      "3: T → a\n",
      "Enter <<fun:ProdT@18>> with token `A`\n",
      "Match A with A\n",
      "Exit <<fun:ProdT@18>> with token `PLUS`\n",
      "Exit <<fun:ProdS@13-4>> with token `PLUS`\n",
      "Exit <<fun:ProdS@10>> with token `PLUS`\n",
      "Enter <<fun:ProdS@10-1>> with token `PLUS`\n",
      "Match PLUS with PLUS\n",
      "Exit <<fun:ProdS@10-1>> with token `RPAR`\n",
      "Enter <<fun:ProdS@10-2>> with token `RPAR`\n",
      "3: T → a\n",
      "Enter <<fun:ProdT@18>> with token `RPAR`\n",
      "Match A with RPAR\n"
     ]
    },
    {
     "ename": "Unhandled Exception",
     "evalue": "Cannot match symbol `A` with `RPAR`",
     "output_type": "error",
     "traceback": [
      "Unhandled Exception",
      "Cannot match symbol `A` with `RPAR`",
      "   at FSI_0009.Match(TOKEN term, Tokenizer cnxt)",
      "   at FSI_0009.ProdT@18.Invoke(Tokenizer cnxt)",
      "   at FSI_0006.op_EqualsEqualsGreater(Tokenizer cnxt, FSharpFunc`2 prod)",
      "   at FSI_0009.ProdT(Tokenizer cnxt)",
      "   at FSI_0009.ProdS@10-2.Invoke(Tokenizer cnxt)",
      "   at FSI_0006.op_EqualsEqualsGreater(Tokenizer cnxt, FSharpFunc`2 prod)",
      "   at FSI_0009.ProdS(Tokenizer cnxt)",
      "   at FSI_0009.ProdS@10.Invoke(Tokenizer cnxt)",
      "   at FSI_0006.op_EqualsEqualsGreater(Tokenizer cnxt, FSharpFunc`2 prod)",
      "   at FSI_0009.ProdS(Tokenizer cnxt)",
      "   at FSI_0012.it@1-1.Invoke(Tokenizer cnxt)",
      "   at FSI_0006.op_EqualsEqualsGreater(Tokenizer cnxt, FSharpFunc`2 prod)",
      "   at <StartupCode$FSI_0012>.$FSI_0012.main@()"
     ]
    }
   ],
   "source": [
    "Tokenizer(grammar, true, InputState=\"((a+)+a)\") |> parser |> ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (F#)",
   "language": "F#",
   "name": ".net-fsharp"
  },
  "language_info": {
   "file_extension": ".fs",
   "mimetype": "text/x-fsharp",
   "name": "C#",
   "pygments_lexer": "fsharp",
   "version": "4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
